{"/posts/":{"data":{"":"RSS Feed"},"title":"Posts"},"/posts/post-0001-a-poor-guys-apm/":{"data":{"":"I‚Äôve seen the benefits of instrumenting code for a very long time: with PHP, Go, Frontend Javascript, and more recently, NodeJS.\nWhen you have access to a full APM solution like I do at work (Newrelic), or on the side (with Elastic APM ), it becomes easy to see at a glance not only what transactions (API calls, queued jobs, asynchronous processes, etc) are taking a long time, but also why.\nI ran into a situation where one of my first projects experienced a bottleneck with slow page loads. In Newrelic Lite (the free forever plan), I could tell that on average, we were making select queries on a table over a 100 times per page load on average. Most of this code was written in 2010 (my excuse, although not a very good one), and at a glance it wasn‚Äôt easy to tell where exactly the problem was, although I knew what it was: ORM lazy loading in a loop .\nUpgrading to the a pricier version of Newrelic was not an option. I had a local version of the application working with docker-compose: was there perhaps a solution I could run locally, while pinpointing the exact problem? There had to be.\nI tried:\nXDebug. The holy grail. Grind viewers were less than helpful getting an exact call sequence. Not fun. PHP APM (https://pecl.php.net/package/APM ). This hasn‚Äôt been updated since 2017, fails to build on PHP 7.3 , and I experienced random application crashes on 7.2. Pinpoint APM (https://naver.github.io/pinpoint/ ). Let‚Äôs just say there was a lot of headscratching, especially around the install. I sort of got it working, but didn‚Äôt get any useful data. Also, more random crashes. I tried other APM solutions, most of them Saas, but the hurdle was either installing them, the signup/activation process, or the knowledge that I wouldn‚Äôt be able to reliable diagnose another problem after the trials ran out.\nBut wait. I‚Äôd done some work with OpenTracing and OpenCensus before. Could I‚Ä¶?\nYES! I modified my Dockerfile very simply:\nFROM php:7.3-apache # Everything else RUN apt-get update \u0026\u0026 apt-get install -y zlib1g-dev libzip-dev zip unzip \u0026\u0026 \\ docker-php-ext-install iconv bcmath zip mysqli opcache pdo pdo_mysql \u0026\u0026 \\ echo \"done!\" # OpenCensus RUN curl -L https://github.com/census-instrumentation/opencensus-php/archive/master.zip -o opencensus.zip \u0026\u0026 unzip opencensus.zip RUN (cd opencensus-php-*/ext \u0026\u0026 phpize \u0026\u0026 ./configure --enable-opencensus \u0026\u0026 make \u0026\u0026 make test \u0026\u0026 make install) RUN docker-php-ext-enable opencensus I added both opencensus/opencensus and opencensus/opencensus-exporter-zipkin (for Jaeger ) to composer. I didn‚Äôt use opencensus/opencensus-exporter-jaeger at the time due to this issue .\ncomposer require --dev 'opencensus/opencensus:^0.5.2' 'opencensus/opencensus-exporter-zipkin:^0.1.0' Followed by changing my docker-compose.yml:\nversion: '3.6' services: app: build: . links: - db ports: - '7999:80' expose: - '80' volumes: - .:/var/www/html environment: - DB_HOST=db - DB_NAME=appdb - DB_USER=root - DB_PASS=phpapptest - DB_PORT=3306 - KOHANA_ENV=DEVELOPMENT db: image: 'mariadb:10' ports: - '3360:3306' volumes: - ./build/phpunit_database.sql:/docker-entrypoint-initdb.d/db.sql - ./docker/mysql:/var/lib/mysql environment: MYSQL_ROOT_PASSWORD: phpapptest MYSQL_DATABASE: appdb opencensus: image: jaegertracing/all-in-one:latest environment: COLLECTOR_ZIPKIN_HTTP_PORT: '9411' ports: - 9400:16686 - 9414:9411 expose: - 5775/udp - 6831/udp - 6832/udp - 5778 - 16686 - 14268 - 9411 With that, I had the Jaeger endpoint running at http://localhost:9400\nIn my application entry point, right after including vendor/autoload.php, I added the below:\nThere‚Äôs some gore in there because we‚Äôre in the middle of replacing Kohana Koseven with Laravel .\n\u003c?php use OpenCensus\\Trace\\Span; use OpenCensus\\Trace\\Tracer; use OpenCensus\\Trace\\Exporter\\ZipkinExporter; if (extension_loaded('opencensus')) { // Inbuilt instrumentation OpenCensus\\Trace\\Integrations\\Mysql::load(); OpenCensus\\Trace\\Integrations\\PDO::load(); OpenCensus\\Trace\\Integrations\\Eloquent::load(); OpenCensus\\Trace\\Integrations\\Curl::load(); // Other opencensus_trace_method(trim(PageManager::class, '\\\\'), '__construct', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), '__construct'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(PageManager::class, 'get', function($scope, $id){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'get'), 'attributes' =\u003e compact('id'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(PageManager::class, 'init', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'init'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(PageManager::class, 'top', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'top'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(PageManager::class, 'topMenu', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'topMenu'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); // Views opencensus_trace_method(Kohana_View::class, 'factory', function($scope, $file){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'find_all'), 'attributes' =\u003e array_map('strval', compact('file')), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_View::class, 'render', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'render'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); // Database and ORM opencensus_trace_method(Kohana_ORM::class, 'find_all', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'find_all'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_ORM::class, 'find', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'find'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); $databaseClasses = [Kohana_Database::class, Kohana_Database_MySQLi::class, Database_CMySQLi::class]; foreach ($databaseClasses as $databaseClass) { opencensus_trace_method($databaseClass, 'query', function($scope, $type, $sql){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'query'), 'attributes' =\u003e ['query' =\u003e $sql], 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method($databaseClass, 'begin', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'begin'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method($databaseClass, 'commit', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'commit'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method($databaseClass, 'rollback', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'rollback'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); } // Controllers $controllerClasses = [Kohana_Controller::class, Controller::class, Base_Controller::class]; foreach ($controllerClasses as $controllerClass) { opencensus_trace_method($controllerClass, '__construct', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), '__construct'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method($controllerClass, 'before', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'before'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method($controllerClass, 'after', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'after'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method($controllerClass, 'execute', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'execute'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); } opencensus_trace_method(Controller_Page::class, 'setup', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'setup'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); // Request execution and responses opencensus_trace_method(Kohana_Request::class, 'execute', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'execute'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_Request_Client::class, 'execute', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'execute'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_Request_Client::class, 'execute_request', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'execute_request'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_Request::class, 'process', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'process'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_Response::class, 'body', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'body'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_Response::class, 'send_headers', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'send_headers'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); // Routing opencensus_trace_method(Kohana_Route::class, 'set', function($scope, $name){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'name'), 'attributes' =\u003e array_map('strval', compact('name')), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(Kohana_Route::class, 'cache', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'cache'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); // Laravel opencensus_trace_method(\\CongreGATEv7\\Models\\User::class, 'findForPassport', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'findForPassport'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\CongreGATEv7\\Models\\User::class, 'validateForPassportPasswordGrant', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'validateForPassportPasswordGrant'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\CongreGATEv7\\Models\\User::class, 'getAuthIdentifierName', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'getAuthIdentifierName'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\Illuminate\\Auth\\EloquentUserProvider::class, 'retrieveById', function($scope, $identifier){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'retrieveById'), 'attributes' =\u003e array_map('strval', compact('identifier')), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\Illuminate\\Auth\\EloquentUserProvider::class, 'retrieveByToken', function($scope, $identifier, $token){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'retrieveByToken'), 'attributes' =\u003e array_map('strval', compact('identifier', 'token')), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\Illuminate\\Auth\\EloquentUserProvider::class, 'updateRememberToken', function($scope, $user, $token){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'updateRememberToken'), 'attributes' =\u003e array_map('strval', compact( 'token')), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\Illuminate\\Auth\\EloquentUserProvider::class, 'retrieveByCredentials', function($scope, $user, $token){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'retrieveByCredentials'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); opencensus_trace_method(\\Illuminate\\Auth\\EloquentUserProvider::class, 'validateCredentials', function($scope){ return [ 'name' =\u003e sprintf('%s::%s', get_class($scope), 'validateCredentials'), 'kind' =\u003e Span::KIND_CLIENT, ]; }); Tracer::start(new ZipkinExporter('congregate', 'http://opencensus:9411/api/v2/spans')); } For a case where I needed a stacktrace (calling debug_backtrace() or debug_print_backtrace() inside one of the scoped lambda functions above WILL crash the application), I had to do something this ugly:\n\u003c?php public function find_all() { if (extension_loaded('opencensus')) { ob_start(); debug_print_backtrace(DEBUG_BACKTRACE_IGNORE_ARGS); $trace = ob_get_clean(); return Tracer::inSpan([ 'name' =\u003e 'Model_Page::find_all.instrumented', 'attributes' =\u003e [ 'trace' =\u003e $trace, ] ], function() { return parent::find_all(); }); } return parent::find_all(); } A couple days ago, I was told calendar feeds were loading very slowly. All I had to do was look in Newrelic:\nNot good. Given these relationships:\nEvent (belongsTo) EventCategory (hasMany) EventUnavailabilities on average we were querying for events 20 times, categories 57 times, unavailabilities (time ranges to exclude events) 359 times, and members and families for birthdays and anniversaries on the member calendar roughly 15-20 times on average. NOT GOOD.\nLooking locally, I saw this which has a whopping 678 spans:\nA quick rewrite later, by prefetching, I got this:\n322 spans, which looks much saner.\nEven Newrelic agrees:\nAnd that, friends, is one way to run a local APM for PHP."},"title":"A Poor Guy's Local PHP Apm"},"/posts/post-0002-that-minor-snafu-with-go-types/":{"data":{"":"Today‚Äôs specimen:\ntype expectedError struct { error } func (e *expectedError) Cause() error { return e.error } func IsExpectedError(err error) bool { _, ok := err.(*expectedError) return ok } In one of my projects, I‚Äôm using consul for distributed locking , to prevent processing an item twice from a queue worker.\nMy implementation looked something like the below:\nimport ( \"context\" \"github.com/hashicorp/consul/api\" \"github.com/pkg/errors\" \"os\" \"time\" ) type lock struct { db datasource.LockStore consul *api.Client } func (l *lock) AcquireLock(ctx context.Context, action string, f func() error) error { lck, err := l.db.GetLock(ctx, action) if err != nil { return errors.Wrap(err, \"unable to query database for lock\") } if lck != nil { return \u0026expectedError{ errors.Errorf(\"item %s already processed at %s\", action, lck.CreatedAt.Format(time.RFC3339)), } } lck = \u0026models.Lock{ Action: action, } // nolint hn, _ := os.Hostname() opts := \u0026api.LockOptions{ Key: lck.Action, Value: []byte(\"set by \" + hn), SessionTTL: \"120s\", LockWaitTime: time.Second * 2, } cLck, err := l.consul.LockOpts(opts) if err != nil { return errors.WithStack(err) } _, err = cLck.Lock(nil) if err != nil { return errors.WithStack(err) } defer func() { _ = cLck.Unlock() }() if err = f(); err != nil { return errors.WithStack(err) } return \u0026expectedError{ errors.Wrapf(l.db.CreateLock(ctx, lck), \"unable to save lock to database for %s at %s\", lck.Action, lck.CreatedAt.Format(time.RFC3339)) } } What could go wrong? Everything.\nSomething like\n// Process message in a lock err = w.lock.AcquireLock(ctx, action, func() error { return w.sendMessage(ctx, m, accountName) }) if err != nil { // This detects a lock-only error and uses that if locking.IsExpectedError(err) { return nil } return err } doSomething() would never execute doSomething(), even if sendMessage() succeeds, because I decided to be lazy and wrap the result of CreateLock() without checking for a non-nil error.\nTiny bug, annoying to troubleshoot, until I wrote tests, which makes me wonder why I didn‚Äôt write any in the first place."},"title":"That Minor SNAFU With Go Types"},"/posts/post-0003-a-billing-problem/":{"data":{"":"My partner and I started SingleSend a little over a year ago.\nEverything we knew about SMS messages was that we had a 160-character limit, so dividing the number of characters in a message by 160 and rounding up would tell us how many messages to bill for. That held true until we got a $750 bill from Twilio last month which didn‚Äôt quite match the sending statistics we saw.\nMy first thought was malicious actors: I immediately rotated our credentials before taking a look at the logs from Twilio which fortunately, are quite exhaustive. We found no rogue phone numbers in use, and could trace the SMS IDs back to individual campaign message details in our database.\nOur Twilio bill did mention something called ‚Äúsegments‚Äù which led us to Twilio‚Äôs good post on the subject . The short summary is: most SMS messages use an encoding format called GSM-7 for which the 160 characters per message rule mostly holds true: however, once you introduce certain special characters, including emojis, the encoding switches to UCS-2 .\nTwilio has a very convenient segment calculator that helped troubleshoot.\nHere are two sample SMS messages:\nHello, this is a very simple message that should fit into exactly one text. Adding a real :) somehow triples the number of segments, which isn't fun to find out\nHello, this is a very simple message that should fit into exactly one text. Adding üò≠ triples the number of segments: finding that out on your bill is quite fun.\nBoth have 160 characters, but the second one contains 3 segments while the first has only 1. When you translate that to a 5000-member campaign, the losses weren‚Äôt pretty. That was a slightly expensive lesson to learn, but it‚Äôs interesting how assumptions like this don‚Äôt get noticed until you put in a multiplying factor.\nI‚Äôll hopefully tell a story about cloud network transfer costs sometime."},"title":"A billing problem based on assumptions"},"/posts/post-0004-fun-with-the-ssh-a-flag/":{"data":{"":"A long, long time ago when my daily driver was a Windows machine, I used to deploy websites over ftp, then sftp. I remember SmartFTP being one of the first software tools I ever purchased.\nIn time (around 2011), I got introduced to Capistrano when I started doing continuous integration/delivery and setting up multiple environments for my projects.\nGetting a consistent ruby ecosystem working for mostly php projects was annoying as I moved between Jenkins, Wercker (pre-Oracle), Shippable (before they got acquired by JFrog) and finally CircleCI. I did have Ruby dependencies because Compass was all the rage, but eventually, it became annoying as browsers stopped needing prefixes, and Sass got ported to other languages.\nI started doing deploys somewhat like this:\n- if [ \"$BRANCH\" != \"master\" ]; then ssh -t $DEPLOY_USER@$DEPLOY_HOST \"cd ~/app_staging \u0026\u0026 git fetch --all \u0026\u0026 git checkout --force $COMMIT \u0026\u0026 php composer.phar install --no-dev --optimize-autoloader\" \u0026\u0026 grunt; curl -H \"x-api-key:xxxx\" -d \"deployment[app_name]=App-other\" -d \"deployment[description]=This deployment can be viewed at ${BUILD_URL}\" -d \"deployment[revision]=${COMMIT}\" -d \"deployment[changelog]=${SHIPPABLE_BUILD_ID}\" -d \"deployment[user]=${USER}\" https://api.newrelic.com/deployments.xml; fi - if [ \"$BRANCH\" == \"master\" ]; then ssh -t $DEPLOY_USER@$DEPLOY_HOST \"cd ~/app_system \u0026\u0026 git fetch --all \u0026\u0026 git checkout --force $COMMIT \u0026\u0026 php composer.phar install --no-dev --optimize-autoloader\" \u0026\u0026 grunt; curl -H \"x-api-key:xxxx\" -d \"deployment[app_name]=App-production\" -d \"deployment[description]=This deployment can be viewed at ${BUILD_URL}\" -d \"deployment[revision]=${COMMIT}\" -d \"deployment[changelog]=${SHIPPABLE_BUILD_ID}\" -d \"deployment[user]=${USER}\" https://api.newrelic.com/deployments.xml; fi I missed capistrano multistage though, because there was no easy way to roll back deployments other than manually, and on projects where I had to deploy to multiple servers, this became unwieldy very quickly.\nEnter deployer .\nIt had an API that reminded me of Capistrano: multiple hosts and stages, with support for custom tasks and symlinked releases that didn‚Äôt get swapped until deployment completion.\nI started at v3 and have gone through 4 major version upgrades with some battle scars to show.\nThe latest one was a simple task definition which I used to keep secrets pulled from a separate repo (admittedly not a very good idea, but one that works for now):\ntask('env:secrets', function () { run('(cd ~/.env-secrets \u0026\u0026 git pull)'); })-\u003edesc('Pull secret files.'); I kept getting a fatal pull error even though I could ssh into all the affected servers and literally run (cd ~/.env-secrets \u0026\u0026 git pull) without any problems. I could also run the deploy command locally without any issues, ie:\n./vendor/bin/dep env:secrets stage=staging --revision=$(git rev-parse HEAD) -vvv Using CircleCI‚Äôs ssh mode to troubleshoot got me nothing. Cue the dreaded list of CI git commits a la try deploy x16.\nI fix CI systems at work quite a bit, and in almost every case, the problem is right there in the terminal output, so it was quite ironic when I took a closer look and saw:\nssh '-F' '/home/circleci/.ssh/config' '-A' '-o' 'ControlMaster=auto' '-o' 'ControlPersist=60' '-o' 'ControlPath=/dev/shm/***@****' '***@****' ': 34bf7dc0417ecfc06846; bash -ls' Using those ssh args, and running a git pull, I got the expected fatal error: bingo!\nAfter reading the fine manual and narrowing the culprit to the -A flag , it was as simple as updating the task to:\ntask('env:secrets', function () { run('SSH_AUTH_SOCK=\"\" (cd ~/.env-secrets \u0026\u0026 git pull)'); })-\u003edesc('Pull secret files.'); Deployer does have a -\u003esetForwardAgent(false) host flag, but 4 major upgrades have taught me not to mess too much with the internals when they just work.\nAs always, it‚Äôs the little things that cost you hours of debugging, especially when your internet is spotty."},"title":"Fun with php deployer and ssh"},"/posts/post-0005-at-gophercon/":{"data":{"":"‚ÄúSorry, I‚Äôll have to drop in 8 minutes‚Äù I typed into a dedicated Slack channel at work for a group bug fixing session.\nI was at GopherCon , not quite bright-eyed from a night of wrestling with my hotel‚Äôs internet connection, but looking forward to a Keynote talk about proposed changes to the encoding/json package .\nI had my badge flipped, because I didn‚Äôt want to talk to anyone and was in that little zone where I counted the time down before I had to head back upstairs to the Keynote ballroom.\n‚ÄúAre you Azuka?‚Äù\nI looked up to see someone approaching. He didn‚Äôt look familiar, and for a brief moment I wondered if he was on the buf team. I‚Äôd had a pretty interesting chat the day before about my issues getting connect-es to run in Firebase functions using fastify, as well as some typing issues converting typescript PlainMessage representations to Firestore-compatible models. Turns out I was talking to an ex-Googler who worked on Firestore, but I digress.\n‚ÄúYes I am,‚Äù I replied, not quite able to place him.\n‚ÄúWhy did you attack the CTF server on Tuesday?‚Äù\nThat came completely out of left-field, because the only response I could muster was ‚Äúhuh?‚Äù\n‚ÄúThe Capture The Flag event server‚Äù he continued. ‚ÄúYou attacked the server and took down the community internet on Tuesday.‚Äù\nWhoa. That was a lot to unpack. Surely this was some red team exercise, a prank, or just some guy who was off his rocker, right?\nRight?\nWell, no.\n‚ÄúWhat were you doing between 1:30 and 3* Tuesday afternoon?‚Äù he continued.\n‚ÄúWell, I‚Äôm currently trying to get a containerized application to run on Google Cloud,‚Äù I explained. ‚ÄúI‚Äôve been trying to get it to build but having issues on my Mac M1 due to some of the python scripts somehow not working locally.‚Äù\n‚ÄúI‚Äôm not a python guy,‚Äù at this point I was rambling because his expression told me he didn‚Äôt believe a word I was saying. ‚ÄúIt was set up using something called poetry which was taking a long time to solve dependencies and pull pip packages. I eventually started running and troubleshooting on Google Cloud build. Would you like to see?‚Äù\nHe did.\nI showed him the cloud build timestamps, my git history as well as my local history (thanks Jetbrains / Webstorm). He also collected the host names of my personal and work MacBooks as well as the ifconfig outputs, possibly to match my MAC address. He asked if I had another MacBook, but I told him I didn‚Äôt.\n‚ÄúAre you in the Gophers Slack?‚Äù he asked.\n‚ÄúYes‚Äù I replied.\n‚ÄúWhy didn‚Äôt you respond to the email from H* yesterday?‚Äù he followed up with another question.\n‚ÄúI‚Äôm not sure I got it,‚Äù was my response. Who checks work email when not at work?\n‚ÄúShe sent it to your work email address.‚Äù\nI looked in my inbox, and there it was in the non-focused section:\n‚ÄúREAD ME: GC23 | Need to Speak to You‚Äù\nHardly a title that doesn‚Äôt scream: recruiter.\n‚ÄúOh, it wasn‚Äôt in the focused section in Outlook,‚Äù I explained.\n‚ÄúOkay,‚Äù he gave me a non-committal answer. I asked his name, still convinced this wasn‚Äôt something serious, just so I could look him up. He walked away, I joined the keynote, and that was it.\nOr not.\nI looked up both H and our mystery man who I‚Äôll call B, and confirmed they were both legitimately associated with the conference, so I replied H via email that I‚Äôd be happy to talk if she still wanted to. When I looked in the gophers Slack channel, it turned out she had sent me a direct message there as well, so I told her I‚Äôd replied her email and would be available if she still wanted to talk.\n‚ÄúYes please. Come see me at the tech table after this talk.‚Äù\n‚ÄúOkay.‚Äù\nRight after the talk, I did. I found B there, and together with H and another guy who looked vaguely familiar, but who I‚Äôll call E, we found a corner to talk.\nWhat it boiled down to was that they knew what I‚Äôd ‚Äúdone‚Äù, and the docker stuff I said I was doing didn‚Äôt match the network traffic they saw (‚ÄúI‚Äù‚Äôd apparently hit a peak of somewhere around 300mb/s) which drove the internet bill somewhere into the hundreds of thousands of dollars.\nThen came the usual platitudes of ‚Äúeverybody makes mistakes‚Äù, ‚Äúsome people have done this before and had to be prosecuted‚Äù, and ‚Äúyou could be banned from attending any other event.‚Äù\nI was flummoxed, and starting to get very angry. The problem is when I get angry, I get very calm, and the incredulous looks on their faces told me they were annoyed at how good an actor I was.\nE told me so, and B brandished a printed page with the hostname ‚ÄúAzukas-MBP‚Äù (my hostname is Azukas-Macbook-Pro) and a MAC address as evidence. According to them , they found me Thursday morning because they could track my MacBook based on proximity, and it wasn‚Äôt a mistake that they‚Äôd tracked me down. H said she wasn‚Äôt a technical person, but obviously trusted them, and after going back and forth pretty much told me to ‚Äústay in my lane‚Äù and just not cause trouble for the rest of the conference.\nAt some point, I asked how I could help (network and other logs from my computer, anything) and got the scoffing response that none of them had the time to spend on investigating this. I also remembered a prolonged network blip during the Service Weaver workshop within that timeframe where everybody ( including me) got stuck downloading go modules (go mod tidy) but given the attitude I was being given, I wasn‚Äôt sure about volunteering that information.\nI asked what would happen when or if they eventually discovered I wasn‚Äôt responsible.\n‚ÄúWe would of course tender an apology, but we‚Äôre all in the tech space. These things happen, and it could be a mistake, or you were just playing around, but we just need you to come clean. When you look at the fact you didn‚Äôt reply your email or the direct Slack message until now, it‚Äôs hard not to be suspicious.‚Äù\nIt was a ‚Äúyou know we know that you did it‚Äù situation. The only person who was absolutely sure I knew nothing about this was me, and at 3:1 the odds were just not in my favor.\nAfter another reminder from H to ‚Äústay in my lane‚Äù, we broke up and went back in.\nI enjoyed the encoding/json v2 proposal talk, but when the keynotes ended, I had some time to reflect on everything that transpired and was completely soured on the rest of the conference.\nI sent a quick message to my manager at work just in case, and reached out to a few friends.\nAt this point, I have a lot of questions and no answers.\nWhy were they absolutely sure I was responsible for this? Who actually did it? How was anyone able to generate that kind of traffic? I understand running a conference is hard, but I hope some time is actually taken to look into this and someone reaches out to me.\nI‚Äôll wait.\n*I may have these times wrong.","post-mortem-update-2023-10-26#Post-mortem (update: 2023-10-26)":"It‚Äôs been almost a month since this happened. I wrote this down immediately after, because writing has always been my outlet, but also because I wanted to be sure when I looked back on this in hindsight that my feelings, especially the anger I felt were justified.\nIn the meantime, I did some thinking, as well as some snooping. One thing H kept repeating was that by signing up for the conference, I agreed to abide by the code of conduct at GopherCon. I felt blindsided, since this was three people (from my perspective) starting off with accusing me, and attempting to railroad me into admitting to attacking a network server.\nAt no point was any attempt made to hear my side of the story, other than the beginning accusation by B. I told him about the containers I was building because understandably, that was the only thing I was working on that I believed could increase traffic usage on the network.\nFirst, I went looking for an arbitration process in the GopherCon Code of Conduct , only to find that it was exactly a verbatim copy of the Go Community Code of Conduct , including the email for the Google Open Source Programs Office. I do not believe that Google runs GopherCon so this seemed like there was nothing in place to handle incidents like this, especially give the very unprofessional way I was approached and talked down to.\nLooking in the #gophercon-ctf channel in the Gophers Slack workspace, I saw an exchange:\nB\n2:37 PM Some is attacking https://{redacted}.zip again\n2:37\nIt‚Äôs down again\nB\n2:43 PM Back up\n2:43\nIf anyone identifies who is attacking https://{redacted}.zip you get 1000 points\nSo somewhere in there, someone ‚Äúinvestigated‚Äù and got 1000 points.\nAdditionally, B visited my LinkedIn profile multiple times, although I‚Äôm not sure why.\nThe next day, I sent a Slack message to a group chat with B, H, and E, because I wanted to see if there was a neutral third party who would take a look at what happened, and listen to me.\nAzuka Okuleye 11:47 AM\nHello all.\nI understand running an event is very stressful, but I gave myself some time to sleep on and think over what happened yesterday.\nThis was my very first GopherCon, and I‚Äôm still very upset at being accused of attacking a server. I hope you have some time now with everything over, because this is a very serious allegation and I refuse to let this be swept under the rug.\nI would like to send an email and get this looked into VERY thoroughly because a community is driven not just by successes, but how failures and oversights are handled, and I consider this both. Please let me know who to include.\nThank you.\nVery quickly, I got back a reply:\nH 12:05 PM\nThe email would be to us and our stance hasn‚Äôt changed. We allowed you to stay despite tracking the device to you both days.\nwhich pretty much told me I was not going to get anywhere with them, so I replied with:\nAzuka Okuleye 12:05 PM\nThank you. That‚Äôs all I needed to know.\nE followed up with:\nE 12:33 PM\nAzuka, I wholeheartedly agree about the community aspect. That includes following rules that are in place to not degrade the experience of others.\nYou weren‚Äôt chosen at random. Logs were reviewed, we worked with the hotel‚Äôs networking staff as well. The host name of the machine was your machine. We tracked the MAC address to you. As an IT professional, I‚Äôm sure you can agree that is sufficient evidence to reasonably consider someone a suspect.\nYou suffered no harm. We approached you, asked you about it, informed you that those types of things won‚Äôt be tolerated, and let you know the harm that it cost us and the significant networking expenses that we‚Äôd be charged. It also damages our reputation with the venue. They may not let us come back if issues like this are common. Nothing was publicized, you weren‚Äôt asked to leave, it was a warning.\nYou‚Äôre asking for a ‚ÄúVERY thorough‚Äù investigation, are you sure you know what you‚Äôre asking for? That means working with the hotel‚Äôs network team further, hiring a third-party security team so that it‚Äôs not a conflict, then subpoenaing your devices to have them forensically audited. That will come at a significant cost to us both financially and in time. If that‚Äôs the route we‚Äôre taking, it‚Äôs because we‚Äôre involving the authorities and handling it the legal route. After-all a crime was committed.\nwhich just left me dumbfounded. I suffered no harm, and I was asked, not harangued about it, where the only acceptable answer to them would be me admitting to something I didn‚Äôt do?!\nI followed up with:\nAzuka Okuleye 12:57 PM E, I‚Äôm sure you have a lot of information on your side, and that it‚Äôs my word against what you have found so far.\nI would very likely be inclined to trust it if I were in your position, but I looked in the ctf channel and I know, despite you not believing me, that I did not attack your systems. If, and I know that seems farfetched to you, but if you were in my position and had absolutely no idea what was going on, would you take the word of people who have determined already that you were responsible? I have been on the other side exactly once in my life, and finding out is still one of my lowest moments.\nIs there even a miniscule chance that I‚Äôve been impersonated by someone else? As an IT professional, I‚Äôm not a networking person, but even I know that I wouldn‚Äôt advertise myself using a machine with my name, and that MAC addresses can be spoofed.\nPlease remember you are dealing with a person here. I left the conference yesterday around 3 after getting off the network as soon as we had our talk, but I stayed in San Diego until late hoping against hope that new information would come up, and this would all turn out to be a mistake.\nI offered to help yesterday, and I still stand by that. There‚Äôs surely some middle ground because my experience here is I answered and provided the information B asked for, and would be happy to provide any additional info.\nIf the answer is that you ‚Äúlet‚Äù me stay because it would take considerable effort to look into this, I really hope the next person this happens to gets a better experience than I‚Äôve had.\nSo far, that conversation has gotten no response, which just confirms my gut feeling that all the parties involved at the other end are interested in is for this to just go away.\nThis has been handled very poorly, and from what I see, there‚Äôs little to no accountability here. I think I‚Äôve gone above and beyond, and maybe shows that underneath the veneer of ‚Äúcommunity‚Äù, some people are more equal than others.\nI‚Äôve gotten a few suggestions, but I‚Äôve never liked being the center of attention, so ‚Äútrial by social media‚Äù is unfortunately not where I want this to go.\nIf I‚Äôve shared this with you, please respect that.","update-and-some-closure-2024-05-05#Update and some closure (2024-05-05)":"A day after I posted the post-mortem update above, I sent an email to the addresses on the Go Community Code of Conduct page.\nDate: Fri, 27 Oct 2023 21:53:54 -0700\nSubject: GopherCon Complaint\nFrom: Azuka Okuleye To: conduct@golang.org, opensource@google.com\nHello,\nI found these emails on the Gophercon code of conduct page: https://www.gophercon.com/page/2482900/code-of-conduct .\nAre these the correct places to report a complaint?\nThank you,\nAzuka Okuleye\nHaving received no reply, I sent out another email a month later.\nDate: Sat, 25 Nov 2023 09:30:58 -0800\nSubject: Re: GopherCon Complaint\nFrom: Azuka Okuleye To: conduct@golang.org, opensource@google.com\nGood day,\nI wanted to follow up on this email I sent a month ago.\nBefore I share the details, I wanted confirmation that I‚Äôm reaching out to the right channel (s).\nPlease advise if otherwise.\nThank you.\nAzuka Okuleye\nAt this point, with yet no reply or confirmation, I went with a formal complaint to the Golang Stewards:\nDate: Thu, 30 Nov 2023 07:59:13 -0800\nSubject: Formal Complaint about GopherCON San Diego October 28 2023\nFrom: Azuka Okuleye To: conduct@golang.org\nGood day,\nI am reaching out to make a complaint about how I was treated at the above event.\nOn two occasions, I was approached, first by Benji Vesterby, then by Heather Sullivan, Erik St. Martin and Benji Vesterby and accused of attacking a server used for the Capture the Flag event at the conference. I had no idea about any of this, and I believe I was cooperative in providing any information asked of me.\nInstead, I was subjected to a very disrespectful huddle where I was talked at, not to, and the only thing they wanted from me was a confession to a crime I did not commit. Heather in particular kept repeating that I had agreed to the code of conduct by registering for the conference. I believe that code of conduct goes both ways, especially when the organizers of a conference can make accusations in impunity and refuse any pretense of involving a neutral third party to investigate their claims.\nBenji was outright hostile from the very beginning when he walked up to me and accused me of attacking the server, and I‚Äôm not sure of Erik‚Äôs motivations when he threatens me with either a criminal investigation, or sweeping this under the rug.\nI wrote an account of what happened on my blog an hour later, to ensure I recollected correctly, which you can find replicated below at the end of this email: https://azuka.github.io/posts/post-0005-at-gophercon/ . I have attached relevant screenshots from Slack, including when I reached out to all three people mentioned above.\nI would greatly appreciate a confirmation that this email has been received, even if no immediate action is taken.\nSincerely,\nAzuka Okuleye\nxxx-xxx-xxxx (text-only please, if you need to reach me).\nhttps://www.linkedin.com/in/azuka \u003c everything below here was just copy pasting my entire post before this section \u003e\nEven that got me no reply, so I forwarded to the Google Open Source email address\nDate: Thu, 30 Nov 2023 08:01:17 -0800\nSubject: Fwd: Formal Complaint about GopherCON San Diego October 28 2023\nFrom: Azuka Okuleye To: opensource@google.com\nGood day,\nI hope this finds you well.\nI submitted a formal complaint to the Golang Stewards, which you can find below.\nSincerely,\nAzuka Okuleye\n‚Äî‚Äî‚Äî- Forwarded message ‚Äî‚Äî‚Äî\nFrom: Azuka Okuleye Date: Thu, Nov 30, 2023, 7:59 AM\nSubject: Formal Complaint about GopherCON San Diego October 28 2023\nTo: conduct@golang.org\n\u003c etc, etc \u003e\nThis time I got a response:\nFrom: Russ Cox Date: Mon, 4 Dec 2023 12:43:08 -0500\nSubject: conduct report re GopherCon\nTo: Hi Azuka,\nThank you for reaching out to both conduct@golang.org and opensource@google.com. I wanted to let you know that we received your mail. There was in fact a problem with the conduct@golang.org alias forwarding that we‚Äôve now resolved, so thanks for pinging opensource@ too. I‚Äôm not sure whether I‚Äôll be able to say anything more, but we did receive the message, and I intend to talk to the GopherCon organizers to learn more about the situation. Thanks again.\nBest,\nRuss\nto which I sent a reply.\nDate: Mon, 4 Dec 2023 09:45:39 -0800\nSubject: Re: conduct report re GopherCon\nFrom: Azuka Okuleye To: Russ Cox Hi Russ,\nThank you very much for getting back to me and I appreciate your looking into this.\nBest,\nAzuka\nThen I waited.\nAnd waited.\nAnd waited‚Ä¶\nOne thing I knew would happen is the GopherCon code of conduct had to change. There was no way Google, or the Golang stewards would want people bringing complaints about a conference they did not directly control to them. I checked weekly, sometimes twice a week, and it did change, so I got optimistic.\nI waited three months after my complaint without hearing anything back. In the meantime, I looked at the email on the code of conduct, which pointed to Gopher Academy. Given that the registration , pointed to Erik St. Martin who also (presumably) owns the gopheracademy GitHub organization , I had zero confidence actually following that would yield any useful results.\nOn March 1st, I sent a follow-up email.\nDate: Fri, 1 Mar 2024 13:57:09 -0800\nSubject: Re: Formal Complaint about GopherCON San Diego October 28 2023\nFrom: Azuka Okuleye To: opensource@google.com, conduct@golang.org\nHello,\nI would like to follow up on the status of my complaint.\nI understand you may not be at liberty to share the details, but could you let me know if this is still in progress or if you‚Äôve come to a resolution?\nThank you,\nAzuka Okuleye\nWhich got me nothing.\nOn March 20, I sent a follow-up email to Russ Cox, and got an out of office auto-reply stating he was out until March 26th.\nDate: Wed, 20 Mar 2024 08:04:21 -0700\nSubject: Re: conduct report re GopherCon\nFrom: Azuka Okuleye To: Russ Cox Hi Russ,\nSorry if I‚Äôm bugging you but I just wanted to check back in with you.\nI sent a follow-up email to both conduct@golang.org and opensource@google.com but didn‚Äôt hear back and wanted to confirm if this is still ongoing or if there‚Äôs been a conclusion: understanding, of course that it may not be shared with me.\nThank you,\nAzuka\nApril came around and nothing happened. I had traveled to Chicago for work, and part of that involved containerizing an application while working out dependencies to ensure it could run locally on my work Mac M1s, as well as the Linux AMD64 machines our Kubernetes nodes used. This was almost exactly the same kind of problem I was working on when I got accused.\nI was done being patient, and I wanted this done and gone, so I sent an email.\nDate: Fri, 19 Apr 2024 08:05:44 -0700\nSubject: Re: Formal Complaint about GopherCON San Diego October 28 2023\nFrom: Azuka Okuleye To: opensource@google.com, conduct@golang.org\nGood day,\nI‚Äôm pretty frustrated that I‚Äôve heard nothing back after the initial acknowledgement from Russ Cox.\nAre these emails working? What do I have to do to get a reply?\nAzuka Okuleye\nMy reply came in almost an hour later, which I didn‚Äôt get to reply until I was on the plane headed home.\nFrom: Russ Cox Date: Fri, 19 Apr 2024 12:10:24 -0400\nSubject: Re: [conduct] Re: Formal Complaint about GopherCON San Diego October 28 2023\nTo: Azuka Okuleye Cc: opensource@google.com, conduct@golang.org\nHi Azuka,\nApologies for not getting back to you earlier. I started to reply a few times but always got interrupted.\nI checked in with the organizers, and they shared their point of view and how the events unfolded from their point of view during the conference. In general these kinds of situations are difficult to handle well, and it‚Äôs not uncommon for both sides to feel frustrated with the other side. The organizers said that they don‚Äôt have definitive proof that the attack came from your laptop, only the traffic stats from the hotel‚Äôs wifi system, which showed very significantly more traffic from your laptop than anyone else‚Äôs at the conference. That‚Äôs why they spoke directly to you but did not ask you to leave the conference and did not discuss the incident with anyone else. I can also see that from your point of view it feels like an attack to even be accused.\nUltimately I think it is an unfortunate situation but with little more to be done. The organizers confirmed that you are welcome to attend again.\nI would suggest installing a macOS network traffic monitor such as Little Snitch so that you have more visibility into what your machine is doing on the network. It is possible there was a runaway program doing something that caused all the trouble. More visibility is always a good thing.\nBest,\nRuss\nHonestly, I was very disappointed by the reply. There‚Äôs a very big difference between ‚ÄúWe noticed a lot of traffic coming from your laptop‚Äù and ‚ÄúYou attacked our server‚Äù, especially when I shared willingly that I‚Äôd been trying over and over to build an almost 20gb Dockerfile locally, and running into issues.\nI had a very sarcastic reply typed up almost immediately in Google Keep (for carthasis, I guess), but ultimately sent this out:\nDate: Fri, 19 Apr 2024 18:08:44 -0500\nSubject: Re: [conduct] Re: Formal Complaint about GopherCON San Diego October 28 2023\nFrom: Azuka Okuleye To: Russ Cox Cc: opensource@google.com, conduct@golang.org\nHi Russ,\nThanks for getting back to me. I‚Äôm currently in the middle of traveling so I may be brief.\nI speak very confidently when I say there were better ways to handle this.\nFrom the very beginning when Benji walked up to me with an accusation, to when I later met with Erik and Heather, I was completely on the defensive.\nI dare say any one else in my situation would have frozen up (which I did). I was given no proof or any information: just told to admit to what I did. The implication was that since I did not reply my work email (if only they stopped to consider I was at a conference and away from work) or gopher Slack (which I‚Äôm not active on in the first place), then I had something to hide. I was cooperative, I told them what I‚Äôd been working on, even though it‚Äôs side work that could have gotten me in trouble at my job, I offered my laptop to do any additional digging.\nNobody‚Äôs a villain in their own story, and I‚Äôm sure we all believe we did the best under the circumstances.\nThank you again for bringing this to a conclusion, although not the one I wanted, but you don‚Äôt always get what you want in life. An apology (not from you) would have sufficed.\nI don‚Äôt think I‚Äôll be returning to gopherCon unless I‚Äôm mandated to at work, because as I‚Äôve mentioned before, the handling of this by the organizers and the way I was spoken to will not make me comfortable attending.\nAll the best.\nAzuka\nWell, this is where it all ends, and all these people, as well as GopherCon, can stop living rent-free in my head.\nOh, and here‚Äôs the docker file that ‚Äúattacked‚Äù the precious CTF server, because I‚Äôm still not buying that nonsense:\n# : Where I got stuck because it was initially running on a machine with a gpu #FROM tensorflow/tensorflow:2.4.1-gpu FROM tensorflow/tensorflow:2.13.0 #FROM --platform=linux/x86_64 tensorflow/tensorflow:2.13.0 WORKDIR /app VOLUME /database VOLUME /input VOLUME /logs # : GPG problems when building the image from scratch # https://github.com/tensorflow/tensorflow/issues/56085 RUN apt-key adv --fetch-keys https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/3bf863cc.pub # Install system requirements. RUN apt-get update \u0026\u0026 apt-get upgrade -y \u0026\u0026 apt-get install -y \\ git \\ libfontconfig \\ fontconfig \\ libjpeg-turbo8 \\ libxrender1 \\ xfonts-75dpi \\ xvfb \\ wget \\ \u0026\u0026 rm -rf /var/lib/apt/lists/* # We build wkhtmltox ourselves to include patched QT. COPY backend/deps ./deps RUN dpkg -i ./deps/wkhtmltox_0.12.5-1.bionic_amd64.deb # Copy large binary file requirements early to take advantage of Docker layer # caching. COPY backend/bin ./bin # : Everything here before dumb-init was me fighting python deps. I'm not a python guy # Install poetry RUN pip install --upgrade pip RUN python3 -m pip install --upgrade pip RUN pip install poetry==1.1.15 RUN poetry config virtualenvs.create false COPY backend/pyproject.toml ./pyproject.toml RUN rm -rfv $(poetry config virtualenvs.path)/* #RUN poetry export --no-dev -f requirements.txt --output requirements.txt #RUN pip install -r requirements.txt RUN poetry install --no-dev -vvv RUN poetry run pip install --find-links=deps torchvision #RUN python3 -m spacy download en_core_web_trf #RUN python3 -m spacy download en_core_web_sm # # Install python requirements with pipenv # #RUN pip install setuptools --upgrade # RUN pip install --upgrade pip # RUN pip install pipenv # #COPY backend/pyproject.toml ./pyproject.toml # COPY backend/Pipfile . # # We reuse tensorflow already installed in the base Docker image. # RUN sed -i '/tensorflow\\ =/d' ./Pipfile # RUN pipenv install # #RUN python3 -m pipenv sync --deploy --system # RUN pip install --find-links=deps torchvision RUN python3 -m spacy download en_core_web_trf #RUN python3 -m spacy download en_core_web_sm RUN pip install psycopg2-binary pyparsing RUN wget -O /usr/local/bin/dumb-init https://github.com/Yelp/dumb-init/releases/download/v1.2.5/dumb-init_1.2.5_x86_64 RUN chmod +x /usr/local/bin/dumb-init ENTRYPOINT [\"/usr/local/bin/dumb-init\", \"-v\", \"--\"] # We copy individual folders to preserve directory structure. COPY backend/README.md ./README.md COPY backend/report_templates ./report_templates COPY backend/runtime ./runtime #COPY backend/bin ./bin COPY backend/lib ./lib COPY backend/config ./config #COPY backend/rad_parser ./rad_parser COPY backend/app.py . COPY types/app-report.schema.json ../types/app-report.schema.json ENV ENV=\"dev-docker\" #ENTRYPOINT [\"python3\", \"app.py\"] CMD [\"python\", \"app.py\"] #CMD [\"python3\", \"-m\", \"pipenv\", \"run\", \"python\", \"app.py\"] #CMD [\"python3\", \"-m\", \"trace\", \"--trace\", \"app.py\"]"},"title":"At GopherCon"},"/posts/post-0008-scaling-legacy-congregate/":{"data":{"":"In April 2009, I was a junior at the now defunct Mountain State University in West Virginia. At the time I made a modest freelance income customizing themes and plugins for some of the common PHP CMSes at the time, especially WordPress.\nI took on a project to customize WordPress for a church website: normal CMS functions with a member directory, job scheduling and reminders, a calendar and members‚Äô listing. It was hard work and severely tested, and perhaps contributed to my ability to find answers to questions. Stack Overflow was very new, so in some cases I was the one answering questions, a lot of which haven‚Äôt held up over the years üòÖ.\nOver time, this grew into a product called Congregate . It left its WordPress origins within the first three years and was rewritten into a standalone full CMS using the defunct Kohana framework . By 2020, mostly through word of mouth, we had over 400 sites hosted. When the pandemic hit, load increased because a lot of the churches using the system had to deliver a lot of content virtually. Messaging (text, voice, push, email) exploded, and there were a lot more users accessing the site.\nThe aging deployment architecture was starting to show cracks, but the UX was also starting to turn users off. What used to be modern design in 2012 just wasn‚Äôt cutting it in 2019. One of my big projects around then was creating an entire API layer so the application contents could be used in a mobile application, but also to potentially decouple the presentation layer from the application logic. This would allow people other than me to build entire UIs using modern technologies while the application layer could evolve separately.","admin-and-dns-management#Admin and DNS Management":"The admin interface was a simple Angular application backed by a ConnectRPC API interface.\nThe Admin API uses the Cloudflare API to create and delete DNS records, as well as the Kubernetes API to manage gateway http routes and attached certmanager issuers .","cloud-readiness-and-feature-parity#Cloud readiness and feature parity":"I had a few things to do before even switching the deployment mechanism:\nUnify the database layer: I alluded earlier to the unholy mash of two different frameworks. The database in particular was a big one: Laravel used a PDO connection while the rest of the site used the php mysqli extension. I had to switch this so the database was Laravel-first, and wrote a custom driver to either use the existing PDO connection or initialize it on demand. Remove all local file actions: The application was littered with file_exists() checks for the earliest features where files were still stored on the servers e.g. for directory photos. All local files were synced to S3, and the database tables for the models had an additional JSON or boolean column to track file existence. Unify the cache layer: Similar to the database, we had two different cache stores. This was unified, and in the cloud version we set up a small Elasticache Valkey instance to serve as a distributed cache. Create a Laravel entrypoint: Instead of loading Laravel after Kohana, I moved all of Kohana‚Äôs initialization into a service that got loaded for fallback routes, but also extensively used Laravel‚Äôs service container to initialize on demand e.g. for logic in the calendar that hadn‚Äôt yet been ported over. On the VPS version (because it worked), the application was Kohana first, then Laravel, while in the cloud version, it was Laravel first, then Kohana. All of these required bolstering the Codeception test suite to catch any regressions as well as to cover affected code paths. We went from about 350 tests to ~680.","cloudflare-as-the-dns-provider#Cloudflare as the DNS provider":"I was initially going to do some work with Route53 but when I ran the costs for hosted zones, I started to look elsewhere.\nAs far as I knew, there was no official limit for Cloudflare with their free plan, and I‚Äôd seen numbers in the thousands thrown around, which we weren‚Äôt in any danger of exceeding: at last count, we were still under 600 sites.\nConsidering we‚Äôd get universal SSL, and a world-class WAF at no cost, along with a well-documented API and SDK clients, this seemed like a winner.","communication#Communication":"Eric sent out a few communication messages to site owners with instructions on switching DNS servers. Cloudflare limits the [number of inactive zones](https://community.cloudflare.com/t/stuck-on-you-have-exceeded-the-limit-for-adding-zones-please-activate-some-zones/756329] so communication had to go out in batches until we hit a critical mass.","conclusion#Conclusion":"This was an exhilarating learning experience, and I‚Äôm glad I got to do it. I‚Äôm sure some of this will change, and if interesting enough, I‚Äôll write about them.","eks-as-the-container-orchestration-platform#EKS as the container orchestration platform":"The terraform-aws-modules/terraform-aws-eks terraform module had almost everything needed to setup a cluster. We used Graviton-based instances (a mix between t4g.xlarge and r6g.large, thanks instances.vantage.sh ) and setup ECR registries for the different applications.\nI had setup kubernetes clusters before on a range of cloud providers, but nothing quite prepared me for EKS. Access entries , addon management , IAM roles for service accounts , configuring the IAM OIDC provider for GitHub were just a few things I had to learn about / find. I will eventually go back and properly implement pod identity , maybe‚Ä¶\nThrow in a few modules that should have been addons (they frequently are elsewhere) like the AWS Load Balancer Controller , Secrets Store CSI Driver + AWS Secret Store Provider , Cluster Autoscaler and Descheduler , and cluster setup cost me a solid weekend and some.\nShoutout to cloudposse/vpc-peering/aws which saved me from a long session of setting up VPC peering between the new cloud infrastructure and the default VPC where the existing database lived.\nI did not want to manage or do any work with a CNI plugin yet, so I just went with the AWS VPC CNI , learning about prefix delegation along the way when nodes refused to schedule pods even with available resources.","envoy-gateway--aws-nlb-for-load-balancing#Envoy Gateway + AWS NLB for load balancing":"The major challenge faced with routing and the DNS setup here was that we needed a static set of IPs for Cloudflare or whichever DNS provider we would use. As of the writing of this article, the recommendation from AWS is to assign static IPs to an NLB, with an ALB serving as a target, or to use AWS Global Accelerator.\nWe definitely did not (for cost reasons) want to pay for Global Accelerator or for two load balancers.\nAdditionally, the Ingress API for Kubernetes has been frozen, and I was itching to try out the new Gateway API , eventually picking Envoy Gateway running behind a single NLB.\nWe‚Äôre also running Coraza via an Envoy Extension Policy as a fallback WAF .","grafana-cloud-for-monitoring#Grafana Cloud for monitoring":"Grafana Cloud probably has the best free plan you can get from an observability platform.\nIt does get a bit overwhelming though, and what I‚Äôve learned should probably go into a separate post. I used the k8s-monitoring chart to deploy via Helm, the grafana provider to set up applications, alerts and synthetics, and the new grafana-adaptive-metrics provider to cut down on metrics ingestion.","implement-multi-tenancy#Implement Multi-tenancy":"A lot of the work here was done by the Tenancy for Laravel package. There were a few minor issues that involved tweaking the service container to reset singleton services when switching between tenants.\nI also wrote a script as part of the regular cron process to self-register all the sites as tenants in the admin database.\nEvery site has a corresponding gateway route pointing to the application service, as well as a TLS configuration to generate an SSL certificate with Let‚Äôs Encrypt.","implementation#Implementation":"","monitoring#Monitoring":"All logs and traces are sent to Grafana Cloud using the k8s-monitoring chart. We generate a lot of data, especially metrics and tracing, but I‚Äôve been able to tune the sampling rate. Grafana has also released a few adaptive telemetry options which help filter out unwanted data.\nWe have Grafana synthetic checks set up to monitor the DNS, latency and ssl certificate expiration for which all the alerts go to a dedicated Slack channel.\nWe setup a status page to inform customers of incidents and scheduled maintenance which also tracks some of the downstream services we use.\nGrafana Cloud also has incident response and management , as well as SLO management tools that I hope to explore and use.","preamble#Preamble":"I can use the fancy word multitenant now, but Congregate had its roots in WordPress, which meant every site was provisioned separately, and pointed to a different database. Over time, we moved from shared hosting to a dedicated VPS, split out the database server (which eventually moved to Aurora RDS when I hit my limit managing it), and over time had clients distributed across one of three VPS servers hosted on LiquidWeb , and running CPanel/WHM .\nThe servers had to be scaled and provisioned separately depending on usage, and for some of the larger clients, we would move them over to the newest server (server3) which was provisioned with more vCPUs and RAM. We were tied to CPanel/WHM.\nProvisioning was simply the owner making a new folder, creating a site in WHM/CPanel and pointing to that folder. He would also make a database copy for the new site. A config.php file lived in the folder which contained said database name, and the index.php file simply included a symlinked file that loaded the rest of the application code. A simplified deployment diagram looked like this:\nflowchart TD subgraph whm1[\"WHM/CPanel\"] app1([\"Congregate PHP\"]) dns1([\"DNS *.congregateclients.com\"]) cron1[\"Cron scheduled process\"] end subgraph server1[\"server1\"] whm1 end subgraph whm2[\"WHM/CPanel\"] app2([\"Congregate PHP\"]) dns2([\"DNS *.b.congregateclients.com\"]) cron2[\"Cron scheduled process\"] end subgraph server2[\"server2\"] whm2 end subgraph whm3[\"WHM/CPanel\"] app3([\"Congregate PHP\"]) dns3([\"DNS *.c.congregateclients.com\"]) end subgraph server3[\"server3\"] whm3 end subgraph liquidweb[\"LiquidWeb\"] server1 server2 server3 end subgraph external site1[\"Site 1\"] site2[\"Site 2\"] site3[\"Site 3\"] end site1 -- served from --\u003e server1 site2 -- served from --\u003e server2 site3 -- served from --\u003e server3 I had a pretty good CI/CD configuration using Bitbucket/CircleCI which ran a lot of unit (with PHPUnit) and acceptance/API (using Codeception) tests. Logs were piped to Loggly (acquired by SolarWinds in 2018), so troubleshooting was somewhat manageable. I shared this with the owner:\nThe current application is treated as multiple applications deployed to multiple servers. A few weaknesses of this approach:\nEvery site‚Äôs database has to be set up manually and configured separately using config files. Since each site is tied to the server it‚Äôs managed on, DNS configuration is difficult, with server1 hosting the name servers for the congregateclients.com domain, and delegating b.congregateclients.com to server2 and c.congregateclients.com to server3 respectively Due to how the servers were set up (mostly manually), setting up a new server if we need to scale would require trial and error to ensure they are configured the same way. I have some written notes, but we have made many changes to tune PHP/Apache and the log setup over the years that would involve a lot of trial and error to get working. We are limited by the servers running on Centos 7 which is no longer supported by CPanel ( see https://support.cpanel.net/hc/en-us/articles/19649329138327-When-will-cPanel-stop-supporting-CentOS-7 ) It is difficult to synchronize changes across multiple servers. The deploy process uses a lot of resources, because we: Deploy the code to each server via ssh/sftp Swap out a symlink between the old and new versions of the code Build any UI packages (like the theme selector for the admin) Run migrations across all sites which potentially causes the server to become unresponsive because we start one background process PER site We currently run into performance issues because every site runs a different php process. php-fpm (used on the demo site) is very performant, but due to how CPanel/WHM is set up, dedicating one fpm process per site would mean scaling the servers up to a ridiculous 128GB RAM per server instead of about 12-16GB. In addition to these:\nPHP was locked at version 8.3. We could not upgrade the Centos servers so we couldn‚Äôt take advantage of any security / performance improvements as well as new features in 8.4, with 8.5 just around the corner.","rollout#Rollout":"","scaling#Scaling":"We initially started out with the nginx+php-fpm variant of serversideup/php. We scaled up and benchmarked with k6 and Artillery but kept getting high cpu usage. The directory pdf generation had to be moved to a background process, but we were also able to tune the HPA and resource requests/limits for both the Envoy Gateway pods and the application to handle the load.\nFor the application, we got better performance and resource usage with the Nginx Unit variant.\nSadly, Nginx Unit has been archived , and is no longer maintained. I‚Äôve heard some good things about FrankenPHP , which is supported in the upcoming v4 version of the php images.","serversideupphp-for-base-images#serversideup/php for base images":"I think I found serversideup/php when I was having trouble building an image with the php opentelemetry extension. It saved a lot of effort that would have gone into managing the extension install but also configuring the application server variant (Apache, Nginx+fpm, or Nginx Unit).","so-what-was-the-trigger#So what was the trigger?":"Things came to a head at the beginning of 2025. LLMs had exploded the year before, but 2025 was when AI tools hit critical mass and the companies behind them began to hungrily scrape data from the Internet.\nWe were hit on two fronts: CloudFront daily bills suddenly spiked from less than $4/day to over $200/day, and the VPSes were experiencing massive load, leading to slow loading sites and outright outages.\nWe had AWS WAF deployed in front of the CloudFront CDN, but the basic blocking rules were just not ready for the deluge of requests. I spent my evenings after work searching for patterns in AWS Athena (from CloudFront logs) and tweaking WAF rules with terraform. We had to turn on bot control , block whole ip ranges and countries, then get creative with counting rules when we inevitably started blocking legitimate access to large (mostly mp3) files from podcast aggregators and real users, or images from email clients. Luckily, when we reached out to AWS Support, we were able to get a refund/account credit, with the only requirement being that we turn on AWS Cost Anomaly Detection .\nLiquidWeb, our host had to implement site-wide mod_security rules which caused an incident , and Cloudflare announced pay per crawl around the same time. Clearly, everyone was affected.\nOnce we blocked large file access, the crawlers started to target text content and our RDS external data transfer costs spiked heavily given every page load pulls data from the database. LiquidWeb was able to identify abusive IPs but we were playing whackamole with these, and there was always a new set when we asked. Clearly, this couldn‚Äôt go on.\nEric, the owner of the application asked what could be done on our side, so I sent the diagram below, along with a proposal:\n--- config: layout: elk --- flowchart TD lb[\"Load Balancer\"] subgraph aws db[(\"admin database\")] end subgraph cloud[\"Cloud, e.g. DigitalOcean\"] cluster dns(\"DNS Management\") end subgraph cluster node1 node2 node3 end subgraph node1[\"node1\"] app1([\"Congregate PHP FPM\"]) admin1([\"Admin\"]) cron1[\"Cron scheduled process\"] end subgraph node2[\"node2\"] app2([\"Congregate PHP\"]) end subgraph node3[\"node3\"] app3([\"Congregate PHP\"]) end subgraph external site1[\"Site 1\"] site2[\"Site 2\"] site3[\"Site 3\"] end site1 -- served from --\u003e lb site2 -- served from --\u003e lb site3 -- served from --\u003e lb lb -- routes to --\u003e app1 \u0026 app2 \u0026 app3 app1 \u0026 app2 \u0026 app3 -- determines and serves client based on domain --\u003e db admin1 -- manages client data --\u003e db admin1 -- manages dns records --\u003e dns What the new design achieves is:\nWe can make the application more performant because we can easily scale up and down to handle peak loads (e.g. on Sundays) We can isolate our infrastructure from public access We can have a centralized database where client data lives, which allows us to build an admin panel to manage client domains, as well as to instantly provision a new site and turn sites on / off. We can also centralize the management of configuration instead of needing to log into every site as a sysop user, e.g. for configuring modules and features. It becomes easier to monitor the sites as we can centralize logging and error reporting (I have a few providers we can use where we fall within the free tier, similar to ). To build this out, we will need a minimal viable setup that demonstrates:\nAn Admin API for client management A cloud provider with api support for DNS management and Kubernetes (which we‚Äôll use for deploying the application). We‚Äôll need support for scalable load balancers which we can point the DNS records to. A deployment pipeline from GitHub to the setup. We‚Äôll also need to solve the case where we load directory photos from the file system. This is because we use them in the vcards / pdfs, but we can change how this is done. 1-2 sites we can configure through the admin api to automatically sync dns to the cloud provider. Honestly, I wasn‚Äôt a 100% positive this would work. We had three types of sites hosted on the platform:\nDomain pointed to one of the server ips. Domain using one of our vanity nameservers (ns*.congregateclients.com) which resolved to one of the servers. One of the above, but with a subdomain pointed to the application on one of the servers. I‚Äôd built an entire SSO system for a WordPress plugin (developed by someone else) which allowed churches more flexibility with their site design while integrating with the Congregate API to fetch data. I had the vague idea that I could start on DigitalOcean (for cost reasons), then switch over to AWS when ready, but luckily after a couple discussions, it made sense to go with AWS all the way, especially when factoring in the cost of rewriting API calls that weren‚Äôt a one-to-one mapping between both cloud services. Most of our cloud infrastructure was already hosted there anyway:\nS3 for file storage CloudFront as a CDN Aurora RDS as the primary data store AWS Transcribe Jobs for sermons audio to speech SQS for message queues and background processing","solution#Solution":"This seemed like a perfect use-case for Kubernetes, specifically AWS EKS. Containerization would resolve all (or most of) our dependency issues, and I was already familiar with a few open source projects like cert-manager for dynamic SSL certificate provisioning and external-dns with AWS Route53 integration. This seemed doable and I was excited.\nThe servers + backup on LiquidWeb were costing somewhere around $170/month per server, and optimistically, we could get our costs somewhere around $450/month while future-proofing the application and infrastructure.\nThis was where we ended up:\n--- config: layout: elk elk: mergeEdges: true nodePlacementStrategy: LINEAR_SEGMENTS --- flowchart TD subgraph db[\"RDS Cluster\"] sitedbs[(Site Databases)] admindb[(Admin Database)] end subgraph defaultvpc[\"Default VPC\"] db end subgraph aws defaultvpc cloudvpc end subgraph gw[\"Envoy Gateway\"] lb[\"Load Balancer Service\"] gateway[\"Global Gateway\"] tls[\"TLS Certs\"] end subgraph nodes awsalb[\"AWS ALB Controller\"] cm[\"Cert Manager\"] gw app([\"App deployment and HPA\"]) svc([\"App Service\"]) adminsvc([\"Admin Service\"]) admin([\"Admin deployment and HPA\"]) cron1[\"App CronJob\"] apphttproutes[\"App HTTP Routes\"] adminhttproutes[\"Admin HTTP Routes\"] awsalb -- provisions --\u003e nlb cm -- provisions --\u003e tls svc --\u003e app adminsvc --\u003e admin lb -- routes to --\u003e gateway end subgraph asg[\"Autoscaling Group\"] nodes end subgraph cluster[\"EKS Cluster\"] asg end subgraph cloudflare subgraph appdomains[\"App Domains\"] site1[\"Site 1\"] site2[\"Site 2\"] site3[\"Site 3\"] end adminsite[\"Admin Site\"] end subgraph cloudvpc[\"Cloud VPC\"] cluster nlb[\"Network Load Balancer\"] end defaultvpc \u003c--- peering --\u003e cloudvpc tls -- used by --- apphttproutes \u0026 adminhttproutes site1 \u0026 site2 \u0026 site3 \u0026 adminsite -- served from --\u003e nlb nlb -- routes to --\u003e lb gateway -- routes to --\u003e apphttproutes \u0026 adminhttproutes apphttproutes -- routes to --\u003e svc adminhttproutes -- routes to --\u003e adminsvc app -- determines and serves client based on domain --\u003e sitedbs admin -- manages client data --\u003e admindb admin -- manages dns records ----\u003e appdomains There were a few moving pieces that would make this possible:","tenancy-for-laravel-as-the-tenancy-resolver#Tenancy for Laravel as the tenancy resolver":"I have written multitenancy systems before, and I really did not want to have to write another one for this.\nLuckily, back when I created the API layer, I had created an unholy mash between the newer and more modern Laravel framework and the original Kohana. Newer route endpoints used Laravel‚Äôs routing system, but the framework was initialized after Kohana. It was messy but it worked.\nThe Tenancy for Laravel had almost everything I wanted, and for anything that wasn‚Äôt present, the documentation was very clear about how to implement them."},"title":"Modernizing an aging PHP stack"}}